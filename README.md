# LLM Chat UI

A chat interface for interacting with a Large Language Model (LLM), built with **React** and a basic **Node.js (Express)** backend.  
Currently, the app uses real model-generated replies for prompt-based conversations, while mock replies are used only for testing database-related messages.

---

## Features

- Responsive chat UI (desktop & mobile)
- User and bot message styling
- Smooth text input with Enter-to-send
- Modular component structure (Banner, ChatInput, MessageBubble), each with its own CSS Module for scoped styling
- Basic backend server (`server.js`) ready for API requests
- Easy local development (frontend + backend) with **one command**

---

## Getting Started

### Prerequisites

- [Node.js](https://nodejs.org/) (version 18+ recommended)
- [npm](https://www.npmjs.com/)

---

### Installation

Follow these steps to set up the project locally:

1. **Clone the repository:**

```bash
git clone https://github.com/GWeronika/LLM-model-frontend.git
cd LLM-model-frontend
```
2. **Install all dependencies:**

```bash
npm install
```

---

### Running the App

You have two options depending on your preference:

#### Option 1: Start frontend and backend separately

1. **Backend Server:**

Open the first terminal and run:

```bash
npm run server
```

This will start the backend server at **[http://localhost:5000](http://localhost:5000)**.

2. **Frontend (React app):**

Open the second terminal and run:

```bash
npm start
```

This will start the frontend React app at **[http://localhost:3000](http://localhost:3000)**.

The frontend automatically proxies requests to the backend `/chat` endpoint via the proxy setup in `package.json`.

---

#### Option 2: Start frontend and backend together (recommended)

In a single terminal, run:

```bash
npm run dev
```

This command will:

* Start the backend server at **[http://localhost:5000](http://localhost:5000)**
* Start the frontend React app at **[http://localhost:3000](http://localhost:3000)**
* Proxy API requests between frontend and backend

*This uses the `concurrently` package to run both backend and frontend simultaneously.*

---

## Project Structure

```
LLM-model-frontend/
 ├── public/
 │    ├── img.png
 ├── src/
 │    ├── components/
 │    │    ├── Banner.jsx
 │    │    ├── Banner.module.css
 │    │    ├── CategorySelector.jsx
 │    │    ├── CategorySelector.module.css
 │    │    ├── ChatInput.jsx
 │    │    ├── ChatInput.module.css
 │    │    ├── MessageBubble.jsx
 │    │    ├── MessageBubble.module.css
 │    │    ├── ConversationTitle.jsx
 │    │    ├── ConversationTitle.module.css
 │    │    ├── Sidebar.jsx
 │    │    ├── Sidebar.module.css
 │    ├── App.jsx
 │    ├── App.module.css
 │    └── index.js
 ├── server.js (backend server)
 ├── package.json (frontend + backend scripts)
 └── README.md
```

Each major UI component has its own CSS Module (`*.module.css`) to ensure scoped, maintainable styling.

---

## Backend Server (`server.js`)

The backend is an **Express** server that:

* Accepts `POST` requests at `/chat`
* Logs the received user message
* Returns either:

    * A real reply generated by the LLM for prompt-based messages
    * Or a mock reply (`"Mock reply to: your message"`) when handling database-related messages for testing

---

## Current and Future LLM Integration

The app already integrates with a Large Language Model (LLM) to generate responses for user prompts. At the same time, mock replies are used only in specific cases, such as database-related testing.

The current flow:

1. User sends a message from the frontend UI.
2. Frontend POSTs the message to `/chat`.
3. Backend forwards the message to the LLM or returns a mock reply depending on context.
4. LLM generates a response.
5. Backend sends the response back to the frontend for display.

In the future, this flow will be enhanced to support more complex interactions, improved error handling, message history saving, and additional features.

Example API call handler:

```javascript
const handleSendMessage = async (text) => {
  const userMessage = { sender: 'user', text };
  setMessages((prev) => [...prev, userMessage]);

  try {
    const response = await fetch('/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ message: text }),
    });

    const data = await response.json();
    const botMessage = { sender: 'bot', text: data.reply };
    setMessages((prev) => [...prev, botMessage]);
  } catch (error) {
    console.error('Failed to fetch bot reply:', error);
  }
};
```
---

## Preview

![img.png](public/img.png)

---

## Available Scripts

| Script           | Description                                                          |
| ---------------- | -------------------------------------------------------------------- |
| `npm start`      | Starts the React frontend at **localhost:3000**                      |
| `npm run server` | Starts the Node.js backend server at **localhost:5000**              |
| `npm run dev`    | Starts both frontend and backend simultaneously using `concurrently` |

---

## TODO

* [ ] Add typing indicator ("Bot is typing...")
* [ ] Improve error handling for API failures
* [ ] Save chat history (e.g., with `localStorage` or database)
* [ ] Enhance mobile UX (animations, keyboard behavior)
